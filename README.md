<p align="center">
  <img src="images/logo.png" alt="SlideGen-Bench Logo" width="400">
</p>

<p align="center">
  <b>ğŸ¯ A Comprehensive Benchmark for Evaluating AI-Generated Presentations</b>
</p>

<p align="center">
  <a href="https://huggingface.co/datasets/Yqy6/SlideGen-Align">
    <img src="https://img.shields.io/badge/ğŸ¤—%20Dataset-SlideGen--Align-yellow" alt="Dataset">
  </a>
  <a href="#license">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  </a>
  <a href="https://www.python.org/downloads/">
    <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  </a>
  <a href="#installation">
    <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg" alt="PRs Welcome">
  </a>
</p>

<p align="center">
  <a href="#-abstract">Abstract</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-evaluation-pipeline">Evaluation</a> â€¢
  <a href="#-slidegen-align-dataset">Dataset</a> â€¢
  <a href="#-license">License</a>
</p>

---

## ğŸ“– Abstract

The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments.

In this paper, we introduce **SlideGen-Bench**, a benchmark designed to evaluate slide generation through a lens of three core principles:

| Principle | Description |
|:---------:|:------------|
| ğŸŒ **Universality** | Unified visual-domain evaluation framework agnostic to generation methods |
| ğŸ“Š **Quantification** | Reproducible metrics across *Content*, *Aesthetics*, and *Editability* |
| âœ… **Reliability** | High correlation with human preference via the SlideGen-Align dataset |

<p align="center">
  <img src="images/main-pipeline.pdf" alt="Main Pipeline" width="800">
</p>

---

## ğŸš€ Installation

```bash
# Clone the repository
git clone https://github.com/yunqiaoyang/SlideGen-Bench.git
cd SlideGen-Bench

# Install dependencies
pip install -r requirements.txt
```

### ğŸ“¦ Additional Setup

Configure **PaddleOCR DocLayout Detection** for layout analysis:
- ğŸ“– [PaddleOCR Documentation](https://www.paddleocr.ai/latest/version3.x/module_usage/layout_detection.html#_4)
- We use the `PP-DocLayout_plus-L` model

---

## ğŸ”¬ Evaluation Pipeline

### ğŸ“‹ Step 1: Slide Generation & Preprocessing

Convert all slide formats into images to ensure a unified evaluation framework.

<details>
<summary><b>ğŸ–¼ï¸ Image Conversion</b></summary>

We provide a converting script for preprocessing:

```bash
python eval/pre_process.py --input /path/to/slides --output /path/to/images
```

For pipelines that do not directly output images:

```bash
python eval/process_zhipu.py  # Example script - adapt to your pipeline
```

</details>

<details>
<summary><b>ğŸ“‘ PPTX to Image Conversion</b></summary>

For PPTX files, we use **LibreOffice** for conversion:
- ğŸ“– [Official LibreOffice Documentation](https://www.libreoffice.org/get-help/documentation/)

</details>

---

### ğŸ“ Step 2: Content Evaluation (QuizBank)

Evaluate content quality using the **QuizBank** methodology:

```bash
# Run content evaluation
python eval/quantitative_eval.py --eval-mode content_only --provider openai

# Calculate quiz accuracy and generate results
python eval/calculate_quiz_accuracy.py --input results/content_eval.json --output results/accuracy_table.csv
```

---

### ğŸ¨ Step 3: Aesthetics Evaluation

#### ğŸ“ Core Aesthetics Metrics

Computational aesthetics metrics for objective evaluation:

```bash
python eval/aesthetics_metrics.py IMAGE_PATH [OPTIONS]
```

| Metric | Description |
|:-------|:------------|
| `figure_ground_contrast` | ğŸ”² Measures foreground/background contrast using WCAG standards |
| `color_harmony` | ğŸ¨ Computes distance to harmonic color templates |
| `colorfulness` | ğŸŒˆ Measures colorfulness using Hasler & SÃ¼sstrunk's method |
| `subband_entropy` | ğŸ“Š Analyzes visual complexity via subband decomposition |
| `visual_hrv` | ğŸ’“ Visual Heart Rate Variability for temporal consistency from subband entropy |

**Example Usage:**

```bash
python eval/quantitative_eval.py --eval-mode aesthetics_only \
    --aesthetics-metrics figure_ground_contrast,color_harmony,colorfulness,subband_entropy
```

> ğŸ“– For detailed configuration, see [Aesthetics Configuration Guide](docs/Aesthetics_config.md)

#### ğŸ¤– LLM-as-Judge Methods

We also provide LLM-based evaluation methods:

| Method | Description |
|:-------|:------------|
| **LLM Rating** | Direct scoring by language models |
| **LLM Arena** | Pairwise comparison with ELO ranking |

> ğŸ“– See [LLM Evaluation Guide](docs/LLM_EVALUATION.md) for detailed documentation

---

### âœï¸ Step 4: Presentation Editability Intelligence (PEI)

Evaluate presentation editability using a **knock-out evaluation strategy** â€” assessing how well generated presentations can be edited and modified after creation.

> ğŸ“„ **Reference:** [PEI Evaluation Protocol](eval/pei.md)

---

## ğŸ“‹ Quick Reference

| Dimension | Method | Script | Description |
|:----------|:-------|:-------|:------------|
| ğŸ“ **Content** | QuizBank | `quantitative_eval.py --eval-mode content_only` | Quiz-based content accuracy |
| ğŸ¨ **Aesthetics** | Computational | `aesthetics_metrics.py` | Objective visual metrics |
| ğŸ¨ **Aesthetics** | LLM Rating | `quantitative_eval.py --eval-mode visual_only` | LLM-based scoring |
| ğŸ¨ **Aesthetics** | LLM Arena | `arena_eval.py` | Pairwise ELO ranking |
| âœï¸ **Editability** | PEI Knock-out | [PEI Protocol](docs/PEI(2).pdf) | Edit capability assessment |

---

## âš™ï¸ Configuration

Configuration options can be set via:
- ğŸ“ Config file: `eval/eval_config.py`
- ğŸ’» Command-line arguments

---

## ğŸ“Š SlideGen-Align Dataset

<p align="center">
  <a href="https://huggingface.co/datasets/Yqy6/SlideGen-Align">
    <img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/dataset-on-hf-sm.svg" alt="Dataset on HF">
  </a>
</p>

We release **SlideGen-Align**, a human preference dataset for evaluating AI-generated slide presentations.

<p align="center">
  ğŸ¤— <a href="https://huggingface.co/datasets/Yqy6/SlideGen-Align"><b>huggingface.co/datasets/Yqy6/SlideGen-Align</b></a>
</p>

### ğŸ“ˆ Dataset Statistics

<table align="center">
  <tr>
    <td align="center"><b>ğŸ“Š Total Rankings</b><br>1,326</td>
    <td align="center"><b>ğŸ¢ Products</b><br>9</td>
    <td align="center"><b>ğŸ“‚ Categories</b><br>7</td>
    <td align="center"><b>ğŸ’¡ Topics</b><br>187</td>
  </tr>
</table>

### ğŸ¢ Products Evaluated

| Product | Provider | Description |
|:--------|:---------|:------------|
| **Gamma** | Gamma.app | ğŸ¨ AI presentation maker |
| **NotebookLM** | Google | ğŸ““ AI notebook with presentation generation |
| **Kimi-Standard** | Moonshot AI | ğŸŒ™ Kimi (standard mode) |
| **Kimi-Smart** | Moonshot AI | ğŸ§  Kimi (smart mode) |
| **Kimi-Banana** | Moonshot AI | ğŸŒ Kimi (Banana template) |
| **Skywork** | Kunlun Tech | ğŸŒ¤ï¸ Skywork AI |
| **Skywork-Banana** | Kunlun Tech | ğŸŒ Skywork (Banana template) |
| **Zhipu** | Zhipu AI | ğŸ¤– Presentation generator |
| **Quake** | ByteDance | âš¡ Quake presentation tool |

### ğŸ“‚ Scenario Categories

| Category | Topics | Description |
|:---------|:------:|:------------|
| `topic_introduction` | 93 | ğŸ“š General topic introductions (AI, Climate Change, 5G, etc.) |
| `product_launch` | 23 | ğŸš€ Product launch announcements |
| `personal_statement` | 20 | ğŸ‘¤ Personal statements and self-introductions |
| `brand_promote` | 15 | ğŸ“¢ Brand promotion and marketing |
| `course_preparation` | 15 | ğŸ“ Educational course materials |
| `work_report` | 13 | ğŸ“Š Work progress reports |
| `business_plan` | 8 | ğŸ’¼ Business plan presentations |

### ğŸ“ Annotation Format

<details>
<summary>Click to expand annotation example</summary>

```json
{
    "results": [
        {
            "product": "NotebookLM",
            "difficulty": "topic_introduction",
            "topic": "FinTech",
            "rank": 1
        },
        ...
    ]
}
```

</details>

### ğŸ’» Usage

```python
from datasets import load_dataset

# Load from Hugging Face
dataset = load_dataset("Yqy6/SlideGen-Align")

# Access the data
for item in dataset['train']:
    print(f"{item['product']} - {item['topic']}: Rank {item['rank']}")
```

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <i>If you find SlideGen-Bench useful, please consider giving us a â­!</i>
</p>
