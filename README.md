<p align="center">
  <img src="images/logo.png" alt="SlidesGen-Bench Logo" width="400">
</p>

<p align="center">
  <b>ğŸ¯ A Comprehensive Benchmark for Evaluating AI-Generated Presentations</b>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2601.09487">
    <img src="https://img.shields.io/badge/ğŸ“„%20Paper-arXiv-b31b1b" alt="Paper">
  </a>
  <a href="https://huggingface.co/datasets/Yqy6/Slides-Align">
    <img src="https://img.shields.io/badge/ğŸ¤—%20Dataset-Slides--Align-yellow" alt="Dataset">
  </a>
  <a href="https://slidesgenbench.yqy314.top/">
    <img src="https://img.shields.io/badge/ğŸŒ%20Homepage-Website-blue" alt="Homepage">
  </a>
  <a href="#license">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  </a>
</p>

<p align="center">
  <a href="#-abstract">Abstract</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-evaluation-pipeline">Evaluation</a> â€¢
  <a href="#-Slides-Align-dataset">Dataset</a> â€¢
  <a href="#-license">License</a>
</p>

---

## ğŸ“– Abstract

The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments.

In this paper, we introduce **SlidesGen-Bench**, a benchmark designed to evaluate slide generation through a lens of three core principles:

| Principle | Description |
|:---------:|:------------|
| ğŸŒ **Universality** | Unified visual-domain evaluation framework agnostic to generation methods |
| ğŸ“Š **Quantification** | Reproducible metrics across *Content*, *Aesthetics*, and *Editability* |
| âœ… **Reliability** | High correlation with human preference via the Slides-Align dataset |

<p align="center">
  <img src="images/main-pipeline.jpg" alt="Main Pipeline" width="800">
</p>

---

## ğŸš€ Installation

```bash
# Clone the repository
git clone https://github.com/yunqiaoyang/SlidesGen-Bench.git
cd SlidesGen-Bench

# Install dependencies
pip install -r requirements.txt
```

### ğŸ“¦ Additional Setup

Configure **PaddleOCR DocLayout Detection** for layout analysis:
- ğŸ“– [PaddleOCR Documentation](https://www.paddleocr.ai/latest/version3.x/module_usage/layout_detection.html#_4)
- We use the `PP-DocLayout_plus-L` model.

ğŸ“‘ PPTX to Image Conversion

For PPTX files, we use **LibreOffice** for conversion:
- ğŸ“– [Official LibreOffice Documentation](https://www.libreoffice.org/get-help/documentation/)

---

## ğŸ”¬ Evaluation Pipeline

### ğŸ“‹ Step 1: Slide Generation & Preprocessing

Convert all slide formats into images to ensure a unified evaluation framework.

<details>
<b>ğŸ–¼ï¸ Image Conversion</b>

We provide a converting script for preprocessing:

```bash
python eval/pre_process.py --ppt-gen-root ./examples/Slides
```

For pipelines that do not directly output images:

```bash
python eval/process_zhipu.py  --product-dir zhipu-product-dir # Example script - adapt to your pipeline
```

</details>


---

### ğŸ“ Step 2: Content Evaluation (QuizBank)

Evaluate content quality using the **QuizBank** methodology:

```bash
# Run content evaluation
python eval/quantitative_eval.py --products Kimi-Standard Kimi-Smart Kimi-Banana NotebookLM Quake Zhipu Gamma Skywork Skywork-Banana --source-mode all --eval-mode content_only --use-grids --workers 4

# Calculate quiz accuracy and generate results
python eval/calculate_quiz_accuracy.py --input results/content_eval.json --quiz-data dataset/slidegen-test.json --output results
```

---

### ğŸ¨ Step 3: Aesthetics Evaluation

#### ğŸ“ Core Aesthetics Metrics

Computational aesthetics metrics for objective evaluation:

```bash
python eval/aesthetics_metrics.py IMAGE_PATH [OPTIONS]
```

| Metric | Description |
|:-------|:------------|
| `figure_ground_contrast` | ğŸ”² Measures foreground/background contrast using WCAG standards |
| `color_harmony` | ğŸ¨ Computes distance to harmonic color templates |
| `colorfulness` | ğŸŒˆ Measures colorfulness using Hasler & SÃ¼sstrunk's method |
| `subband_entropy` | ğŸ“Š Analyzes visual complexity via subband decomposition |
| `visual_hrv` | ğŸ’“ Visual Heart Rate Variability for temporal consistency from subband entropy |

**Example Usage:**

```bash
python eval/quantitative_eval.py --eval-mode aesthetics_only --products Your_product
```

> ğŸ“– For detailed configuration, see [Aesthetics Configuration Guide](docs/Aesthetics_config.md)

#### ğŸ¤– LLM-as-Judge Methods

We also provide LLM-based evaluation methods:

| Method | Description |
|:-------|:------------|
| **LLM Rating** | Direct scoring by language models |
| **LLM Arena** | Pairwise comparison with ELO ranking |

> ğŸ“– See [LLM Evaluation Guide](docs/LLM_EVALUATION.md) for detailed documentation

---

### âœï¸ Step 4: Presentation Editability Intelligence (PEI)

Evaluate presentation editability using a **knock-out evaluation strategy** â€” assessing how well generated presentations can be edited and modified after creation.

> ğŸ“„ **Reference:** [PEI Evaluation Protocol](docs/pei.md)

---

## ğŸ“‹ Quick Reference

| Dimension | Method | Script | Description |
|:----------|:-------|:-------|:------------|
| ğŸ“ **Content** | QuizBank | `quantitative_eval.py --eval-mode content_only` | Quiz-based content accuracy |
| ğŸ¨ **Aesthetics** | Computational | `aesthetics_metrics.py` | Objective visual metrics |
| ğŸ¨ **Aesthetics** | LLM Rating | `quantitative_eval.py --eval-mode visual_only` | LLM-based scoring |
| ğŸ¨ **Aesthetics** | LLM Arena | `arena_eval.py` | Pairwise ELO ranking |
| âœï¸ **Editability** | PEI Knock-out | [PEI Protocol](docs/pei.md) | Edit capability assessment |

---

## ğŸ”§ Adding Your Own Products

You can easily extend SlidesGen-Bench to evaluate your own slide generation product by following these steps:

### 1ï¸âƒ£ Configure Product Settings

Add your product configuration to `eval/eval_config.py` in the `PRODUCTS` dictionary:

```python
PRODUCTS = {
    # ... existing products ...
    "YourProduct": {
        "directory": "YourProduct",           # Folder name in ppt_gen_root
        "input_format": "pptx",               # Format: "pptx" or "pdf"
        "description": "Your product description",
        "structure": "difficulty/topic/name", # Directory structure
        "has_slide_images": False,            # Set True if images pre-extracted
    },
}
```

### 2ï¸âƒ£ Organize Your Slides

Structure your generated slides following the specified format:

```
ppt_gen_root/
â””â”€â”€ YourProduct/
    â””â”€â”€ {difficulty}/              # e.g., topic_introduction, work_report
        â””â”€â”€ {topic}/               # e.g., AI, Climate_Change
            â””â”€â”€ slides.pptx        # Your generated presentation
```

**Supported difficulties:**
- `topic_introduction`
- `work_report`
- `business_plan`
- `brand_promote`
- `personal_statement`
- `product_launch`
- `course_preparation`

### 3ï¸âƒ£ Preprocess Slides

Convert your slides to images for unified evaluation:

```bash
# For PPTX files
python eval/pre_process.py --ppt-gen-root ./your_slides_folder --products YourProduct

# For custom formats, adapt the conversion script:
# python eval/process_custom.py --product-dir your_product_dir
```

### 4ï¸âƒ£ Run Evaluation

Evaluate your product using the benchmark:

```bash
# Content evaluation
python eval/quantitative_eval.py \
  --products YourProduct \
  --eval-mode content_only \
  --workers 4

# Aesthetics evaluation
python eval/quantitative_eval.py \
  --products YourProduct \
  --eval-mode aesthetics_only

# Full evaluation
python eval/quantitative_eval.py \
  --products YourProduct \
  --source-mode all \
  --workers 4
```

### ğŸ“Œ Tips

- âœ… Ensure slide images are stored in `{ppt_path}_images/` folders
- âœ… Use consistent naming conventions across topics
- âœ… Set `has_slide_images: True` if you've already extracted slide images
- âœ… Test with a small subset before running full evaluation

---

## âš™ï¸ Configuration

Configuration options can be set via:
- ğŸ“ Config file: `eval/eval_config.py`
- ğŸ’» Command-line arguments

---

## ğŸ“Š Slides-Align Dataset

<p align="center">
  <a href="https://huggingface.co/datasets/Yqy6/Slides-Align">
    <img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/dataset-on-hf-sm.svg" alt="Dataset on HF">
  </a>
</p>

We release **Slides-Align**, a human preference dataset for evaluating AI-generated slide presentations.

<p align="center">
  ğŸ¤— <a href="https://huggingface.co/datasets/Yqy6/Slides-Align"><b>huggingface.co/datasets/Yqy6/Slides-Align</b></a>
</p>

### ğŸ“ˆ Dataset Statistics

<table align="center">
  <tr>
    <td align="center"><b>ğŸ“Š Total Rankings</b><br>1,326</td>
    <td align="center"><b>ğŸ¢ Products</b><br>9</td>
    <td align="center"><b>ğŸ“‚ Categories</b><br>7</td>
    <td align="center"><b>ğŸ’¡ Topics</b><br>187</td>
  </tr>
</table>

### ğŸ¢ Products Evaluated

| Product | Provider | Description |
|:--------|:---------|:------------|
| **Gamma** | Gamma.com.ai | ğŸ¨ AI presentation maker |
| **NotebookLM** | Google | ğŸ““ AI notebook with presentation generation |
| **Kimi-Standard** | Moonshot AI | ğŸŒ™ Kimi (standard mode) |
| **Kimi-Smart** | Moonshot AI | ğŸ§  Kimi (smart mode) |
| **Kimi-Banana** | Moonshot AI | ğŸŒ Kimi (Banana template) |
| **Skywork** | SKYWORK.ai  | ğŸŒ¤ï¸ Skywork AI |
| **Skywork-Banana** | SKYWORK.ai  | ğŸŒ Skywork (Banana template) |
| **Zhipu** | Zhipu AI | ğŸ¤– Presentation generator |
| **Quark** | Quark AI | âš¡ Quake presentation tool |

### ğŸ“‚ Scenario Categories

| Category | Topics | Description |
|:---------|:------:|:------------|
| `topic_introduction` | 93 | ğŸ“š General topic introductions (AI, Climate Change, 5G, etc.) |
| `product_launch` | 23 | ğŸš€ Product launch announcements |
| `personal_statement` | 20 | ğŸ‘¤ Personal statements and self-introductions |
| `brand_promote` | 15 | ğŸ“¢ Brand promotion and marketing |
| `course_preparation` | 15 | ğŸ“ Educational course materials |
| `work_report` | 13 | ğŸ“Š Work progress reports |
| `business_plan` | 8 | ğŸ’¼ Business plan presentations |

### ğŸ“ Annotation Format

<details>
<summary>Click to expand annotation example</summary>

```json
{
    "results": [
        {
            "product": "NotebookLM",
            "difficulty": "topic_introduction",
            "topic": "FinTech",
            "rank": 1
        },
        ...
    ]
}
```

</details>

### ğŸ’» Usage

```python
from datasets import load_dataset

# Load from Hugging Face
dataset = load_dataset("Yqy6/Slides-Align")

# Access the data
for item in dataset['train']:
    print(f"{item['product']} - {item['topic']}: Rank {item['rank']}")
```

---

## ï¿½ Citation

If you find SlidesGen-Bench useful in your research, please consider citing our paper:

```bibtex
@misc{yang2026slidesgenbenchevaluatingslidesgeneration,
      title={SlidesGen-Bench: Evaluating Slides Generation via Computational and Quantitative Metrics}, 
      author={Yunqiao Yang and Wenbo Li and Houxing Ren and Zimu Lu and Ke Wang and Zhiyuan Huang and Zhuofan Zong and Mingjie Zhan and Hongsheng Li},
      year={2026},
      eprint={2601.09487},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2601.09487}, 
}
```

---

## ï¿½ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <i>If you find SlidesGen-Bench useful, please consider giving us a â­!</i>
</p>
